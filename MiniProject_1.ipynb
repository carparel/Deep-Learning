{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import dlc_practical_prologue as prologue\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_input size = torch.Size([1000, 2, 14, 14])\n",
      "train_target size = torch.Size([1000])\n",
      "train_classes size = torch.Size([1000, 2])\n",
      "test_input size = torch.Size([1000, 2, 14, 14])\n",
      "test_target size = torch.Size([1000])\n",
      "test_classes size = torch.Size([1000, 2])\n"
     ]
    }
   ],
   "source": [
    "nbr_pairs = 1000\n",
    "train_input, train_target, train_classes, test_input, test_target, test_classes = prologue.generate_pair_sets(nbr_pairs)\n",
    "\n",
    "print('train_input size =', train_input.size())\n",
    "print('train_target size =', train_target.size()) #The boolean telling if the two pairs are the same or not \n",
    "print('train_classes size =', train_classes.size())\n",
    "print('test_input size =', test_input.size())\n",
    "print('test_target size =', test_target.size())\n",
    "print('test_classes size =', test_classes.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_NOaux(model, train_input, train_target, nb_epochs, batch_size, criterion, eta): \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = eta)\n",
    "    for e in range(nb_epochs):\n",
    "        if (e % 10 == 0 and e > 0):\n",
    "            eta = eta/10\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr = eta)\n",
    "        for step_ in range(0,train_input.size(0),batch_size):                              \n",
    "            output = model(train_input[step_:step_+batch_size])\n",
    "            loss = criterion(output, train_target[step_:step_+batch_size])\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def train_model_aux(model, train_input, train_target, nb_epochs, batch_size, criterion, eta, lambda_):   \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = eta)\n",
    "    for e in range(nb_epochs):\n",
    "        if (e % 10 == 0 and e > 0):\n",
    "            eta = eta/10\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr = eta)\n",
    "        for step_ in range(0,train_input.size(0),batch_size):                              \n",
    "            output_target, output_im1, output_im2 = model(train_input[step_:step_+batch_size])\n",
    "            loss_target = criterion(output_target, train_target[step_:step_+batch_size])\n",
    "            loss_im1 = criterion(output_im1, train_classes[step_:step_+batch_size,0])\n",
    "            loss_im2 = criterion(output_im2, train_classes[step_:step_+batch_size,1])\n",
    "            loss = loss_target + lambda_*(loss_im1 + loss_im2)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors_NOaux(model, data_input, data_target, mini_batch_size): \n",
    "    nb_errors = 0\n",
    "    for b in range(0, data_input.size(0), mini_batch_size):\n",
    "        output = model(data_input.narrow(0, b, mini_batch_size))\n",
    "        _, predicted_classes = output.data.max(1)\n",
    "        for k in range(mini_batch_size):\n",
    "            if data_target[b + k] != predicted_classes[k]:\n",
    "                nb_errors = nb_errors + 1\n",
    "    return nb_errors/1000\n",
    "\n",
    "def compute_nb_errors_aux(model, data_input, data_target, mini_batch_size): \n",
    "    nb_errors = 0\n",
    "    for b in range(0, data_input.size(0), mini_batch_size):\n",
    "        output,_,_ = model(data_input.narrow(0, b, mini_batch_size))\n",
    "        _, predicted_classes = output.data.max(1)\n",
    "        for k in range(mini_batch_size):\n",
    "            if data_target[b + k] != predicted_classes[k]:\n",
    "                nb_errors = nb_errors + 1\n",
    "    return nb_errors/1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shallow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Shallow_NOsharing_NOaux(nn.Module):\n",
    "    def __init__(self, hidden, act_fun):\n",
    "        super(Shallow_NOsharing_NOaux, self).__init__()\n",
    "        self.act_fun = act_fun\n",
    "        self.fc1_1 = nn.Linear(196, hidden)\n",
    "        self.fc1_2 = nn.Linear(196, hidden)\n",
    "        # After concatenation of the features from image 1 and image 2\n",
    "        self.fc2 = nn.Linear(hidden*2,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_1 = self.act_fun(self.fc1_1(x[:,0,:,:].view(-1,196)))\n",
    "        x_2 = self.act_fun(self.fc1_2(x[:,1,:,:].view(-1,196)))\n",
    "        x = torch.cat([x_1, x_2],1)\n",
    "        x = self.fc2(x)       \n",
    "        return x\n",
    "    \n",
    "class Shallow_sharing_NOaux(nn.Module):\n",
    "    def __init__(self, hidden, act_fun):\n",
    "        super(Shallow_sharing_NOaux, self).__init__()\n",
    "        self.act_fun = act_fun\n",
    "        self.fc1 = nn.Linear(196, hidden)\n",
    "        # After concatenation of the features from image 1 and image 2\n",
    "        self.fc2 = nn.Linear(hidden*2,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        fc_image = []\n",
    "        for image in range(2):\n",
    "            x1 = self.act_fun(self.fc1(x[:,image,:,:].view(-1,196)))\n",
    "            fc_image.append(x1)\n",
    "        x = torch.cat([fc_image[0],fc_image[1]],1)\n",
    "        x = self.fc2(x)       \n",
    "        return x\n",
    "    \n",
    "class Shallow_NOsharing_aux(nn.Module):\n",
    "    def __init__(self, hidden, act_fun):\n",
    "        super(Shallow_NOsharing_aux, self).__init__()\n",
    "        self.act_fun = act_fun\n",
    "        self.fc1_1 = nn.Linear(196, hidden)\n",
    "        self.fc1_2 = nn.Linear(196, hidden)\n",
    "        \n",
    "        # For classification with classes\n",
    "        self.fc_aux1 = nn.Linear(hidden, 10)\n",
    "        self.fc_aux2 = nn.Linear(hidden, 10)\n",
    "        \n",
    "        # After concatenation of the features from image 1 and image 2\n",
    "        self.fc2 = nn.Linear(hidden*2,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.act_fun(self.fc1_1(x[:,0,:,:].view(-1,196)))\n",
    "        x2 = self.act_fun(self.fc1_2(x[:,1,:,:].view(-1,196)))\n",
    "        \n",
    "        aux1 = F.softmax(self.fc_aux1(x1),1)\n",
    "        aux2 = F.softmax(self.fc_aux2(x2),1)\n",
    "        \n",
    "        x = torch.cat([x1, x2],1)\n",
    "        x = self.fc2(x)       \n",
    "        return x, aux1, aux2\n",
    "    \n",
    "class Shallow_sharing_aux(nn.Module):\n",
    "    def __init__(self, hidden, act_fun):\n",
    "        super(Shallow_sharing_aux, self).__init__()\n",
    "        self.act_fun = act_fun\n",
    "        self.fc1 = nn.Linear(196, hidden)\n",
    "        \n",
    "        # For classification with classes\n",
    "        self.fc_aux1 = nn.Linear(hidden, 10)\n",
    "        self.fc_aux2 = nn.Linear(hidden, 10)\n",
    "        \n",
    "        # After concatenation of the features from image 1 and image 2\n",
    "        self.fc2 = nn.Linear(hidden*2,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        fc_image = []\n",
    "        for image in range(2):\n",
    "            x1 = self.act_fun(self.fc1(x[:,image,:,:].view(-1,196)))\n",
    "            fc_image.append(x1)\n",
    "        \n",
    "        aux1 = F.softmax(self.fc_aux1(fc_image[0]),1)\n",
    "        aux2 = F.softmax(self.fc_aux2(fc_image[1]),1)\n",
    "        \n",
    "        x = torch.cat([fc_image[0],fc_image[1]],1)\n",
    "        x = self.fc2(x)       \n",
    "        return x, aux1, aux2      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_NOsharing_NOaux(nn.Module):\n",
    "    def __init__(self, hidden, act_fun):\n",
    "        super(MLP_NOsharing_NOaux, self).__init__()\n",
    "        self.act_fun = act_fun\n",
    "        self.fc1_1 = nn.Linear(196, hidden)\n",
    "        self.fc1_2 = nn.Linear(196, hidden)\n",
    "        self.fc2_1 = nn.Linear(hidden,hidden)\n",
    "        self.fc2_2 = nn.Linear(hidden,hidden)\n",
    "        # After concatenation of the features from image 1 and image 2\n",
    "        self.fc3 = nn.Linear(hidden*2,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1_1 = self.act_fun(self.fc1_1(x[:,0,:,:].view(-1,196)))\n",
    "        x1_2 = self.act_fun(self.fc1_2(x[:,1,:,:].view(-1,196)))\n",
    "        x2_1 = self.act_fun(self.fc2_1(x1_1))\n",
    "        x2_2 = self.act_fun(self.fc2_2(x1_2))\n",
    "        x = torch.cat([x2_1, x2_2],1)\n",
    "        x = self.fc3(x)       \n",
    "        return x\n",
    "\n",
    "class MLP_sharing_NOaux(nn.Module):\n",
    "    def __init__(self, hidden, act_fun):\n",
    "        super(MLP_sharing_NOaux, self).__init__()\n",
    "        self.act_fun = act_fun\n",
    "        self.fc1 = nn.Linear(196, hidden)\n",
    "        self.fc2 = nn.Linear(hidden,hidden)\n",
    "        # After concatenation of the features from image 1 and image 2\n",
    "        self.fc3 = nn.Linear(hidden*2,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        fc_image = []\n",
    "        for image in range(2):\n",
    "            x1 = self.act_fun(self.fc1(x[:,image,:,:].view(-1,196)))\n",
    "            x2 = self.act_fun(self.fc2(x1))\n",
    "            fc_image.append(x2)\n",
    "        x = torch.cat([fc_image[0],fc_image[1]],1)\n",
    "        x = self.fc3(x)       \n",
    "        return x\n",
    "    \n",
    "class MLP_NOsharing_aux(nn.Module):\n",
    "    def __init__(self, hidden, act_fun):\n",
    "        super(MLP_NOsharing_aux, self).__init__()\n",
    "        self.act_fun = act_fun\n",
    "        self.fc1_1 = nn.Linear(196, hidden)\n",
    "        self.fc1_2 = nn.Linear(196, hidden)\n",
    "        self.fc2_1 = nn.Linear(hidden,hidden)\n",
    "        self.fc2_2 = nn.Linear(hidden,hidden)\n",
    "        \n",
    "        # For classification with classes\n",
    "        self.fc_aux1 = nn.Linear(hidden, 10)\n",
    "        self.fc_aux2 = nn.Linear(hidden, 10)\n",
    "        \n",
    "        # After concatenation of the features from image 1 and image 2\n",
    "        self.fc3 = nn.Linear(hidden*2,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1_1 = self.act_fun(self.fc1_1(x[:,0,:,:].view(-1,196)))\n",
    "        x1_2 = self.act_fun(self.fc1_2(x[:,1,:,:].view(-1,196)))\n",
    "        x2_1 = self.act_fun(self.fc2_1(x1_1))\n",
    "        x2_2 = self.act_fun(self.fc2_2(x1_2))\n",
    "        \n",
    "        aux1 = F.softmax(self.fc_aux1(x2_1),1)\n",
    "        aux2 = F.softmax(self.fc_aux2(x2_2),1)\n",
    "        \n",
    "        x = torch.cat([x2_1, x2_2],1)\n",
    "        x = self.fc3(x)       \n",
    "        return x, aux1, aux2\n",
    "    \n",
    "class MLP_sharing_aux(nn.Module):\n",
    "    def __init__(self, hidden, act_fun):\n",
    "        super(MLP_sharing_aux, self).__init__()\n",
    "        self.act_fun = act_fun\n",
    "        self.fc1 = nn.Linear(196, hidden)\n",
    "        self.fc2 = nn.Linear(hidden,hidden)\n",
    "        \n",
    "        # For classification with classes\n",
    "        self.fc_aux1 = nn.Linear(hidden, 10)\n",
    "        self.fc_aux2 = nn.Linear(hidden, 10)\n",
    "        \n",
    "        # After concatenation of the features from image 1 and image 2\n",
    "        self.fc3 = nn.Linear(hidden*2,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        fc_image = []\n",
    "        for image in range(2):\n",
    "            x1 = self.act_fun(self.fc1(x[:,image,:,:].view(-1,196)))\n",
    "            x2 = self.act_fun(self.fc2(x1))\n",
    "            fc_image.append(x2)\n",
    "            \n",
    "        aux1 = F.softmax(self.fc_aux1(fc_image[0]),1)\n",
    "        aux2 = F.softmax(self.fc_aux2(fc_image[1]),1)\n",
    "        \n",
    "        x = torch.cat([fc_image[0],fc_image[1]],1)\n",
    "        x = self.fc3(x)       \n",
    "        return x, aux1, aux2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep_NOsharing_NOaux(nn.Module):\n",
    "    def __init__(self, hidden, act_fun):\n",
    "        super(Deep_NOsharing_NOaux, self).__init__()\n",
    "        self.act_fun = act_fun\n",
    "        self.conv1_1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv1_2 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2_1 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.conv2_2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        \n",
    "        # After concatenation of the features from image 1 and image 2\n",
    "        self.fc1 = nn.Linear(512, hidden)\n",
    "        self.fc2 = nn.Linear(hidden,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1_1 = self.act_fun(F.max_pool2d(self.conv1_1(x[:,0,:,:].view(100,1,14,14)), kernel_size=2, stride=2))\n",
    "        x2_1 = self.act_fun(F.max_pool2d(self.conv2_1(x1_1), kernel_size=2, stride=2))\n",
    "        \n",
    "        x1_2 = self.act_fun(F.max_pool2d(self.conv1_2(x[:,1,:,:].view(100,1,14,14)), kernel_size=2, stride=2))\n",
    "        x2_2 = self.act_fun(F.max_pool2d(self.conv2_2(x1_2), kernel_size=2, stride=2))\n",
    "        \n",
    "        x = torch.cat([x2_1, x2_2],1)\n",
    "        x = self.act_fun(self.fc1(x.view(-1, 512)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class Deep_sharing_NOaux(nn.Module):\n",
    "    def __init__(self, hidden, act_fun):\n",
    "        super(Deep_sharing_NOaux, self).__init__()\n",
    "        self.act_fun = act_fun\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        \n",
    "        # After concatenation of the features from image 1 and image 2\n",
    "        self.fc1 = nn.Linear(512, hidden)\n",
    "        self.fc2 = nn.Linear(hidden,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_images = []\n",
    "        for image in range(2):\n",
    "            first_conv = self.act_fun(F.max_pool2d(self.conv1(x[:,image,:,:].view(100,1,14,14)), kernel_size=2, stride=2))\n",
    "            conv_images.append(self.act_fun(F.max_pool2d(self.conv2(first_conv), kernel_size=2, stride=2)))\n",
    "        \n",
    "        x = torch.cat([conv_images[0], conv_images[1]],1)\n",
    "        x = self.act_fun(self.fc1(x.view(-1, 512)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class Deep_NOsharing_aux(nn.Module):\n",
    "    def __init__(self, hidden, act_fun):\n",
    "        super(Deep_NOsharing_aux, self).__init__()\n",
    "        self.act_fun = act_fun\n",
    "        self.conv1_1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv1_2 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2_1 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.conv2_2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        \n",
    "        # For classification with classes\n",
    "        self.fc_aux1 = nn.Linear(256, 10)\n",
    "        self.fc_aux2 = nn.Linear(256, 10)\n",
    "        \n",
    "        # After concatenation of the features from image 1 and image 2\n",
    "        self.fc1 = nn.Linear(512, hidden)\n",
    "        self.fc2 = nn.Linear(hidden,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1_1 = self.act_fun(F.max_pool2d(self.conv1_1(x[:,0,:,:].view(100,1,14,14)), kernel_size=2, stride=2))\n",
    "        x2_1 = self.act_fun(F.max_pool2d(self.conv2_1(x1_1), kernel_size=2, stride=2))\n",
    "        \n",
    "        x1_2 = self.act_fun(F.max_pool2d(self.conv1_2(x[:,1,:,:].view(100,1,14,14)), kernel_size=2, stride=2))\n",
    "        x2_2 = self.act_fun(F.max_pool2d(self.conv2_2(x1_2), kernel_size=2, stride=2))\n",
    "\n",
    "        aux1 = F.softmax(self.fc_aux1(x2_1.view(-1,256)),1)\n",
    "        aux2 = F.softmax(self.fc_aux2(x2_2.view(-1,256)),1)\n",
    "        \n",
    "        x = torch.cat([x2_1, x2_2],1)\n",
    "        x = self.act_fun(self.fc1(x.view(-1, 512)))\n",
    "        x = self.fc2(x)\n",
    "        return x, aux1, aux2\n",
    "    \n",
    "class Deep_sharing_aux(nn.Module):\n",
    "    def __init__(self, hidden, act_fun):\n",
    "        super(Deep_sharing_aux, self).__init__()\n",
    "        self.act_fun = act_fun\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        \n",
    "        # For classification with classes\n",
    "        self.fc_aux1 = nn.Linear(256, 10)\n",
    "        self.fc_aux2 = nn.Linear(256, 10)\n",
    "        \n",
    "        # After concatenation of the features from image 1 and image 2\n",
    "        self.fc1 = nn.Linear(512, hidden)\n",
    "        self.fc2 = nn.Linear(hidden,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_images = []\n",
    "        for image in range(2):\n",
    "            first_conv = self.act_fun(F.max_pool2d(self.conv1(x[:,image,:,:].view(100,1,14,14)), kernel_size=2, stride=2))\n",
    "            conv_images.append(self.act_fun(F.max_pool2d(self.conv2(first_conv), kernel_size=2, stride=2)))\n",
    "            \n",
    "        aux1 = F.softmax(self.fc_aux1(conv_images[0].view(-1,256)),1)\n",
    "        aux2 = F.softmax(self.fc_aux2(conv_images[1].view(-1,256)),1)\n",
    "        \n",
    "        x = torch.cat([conv_images[0], conv_images[1]],1)\n",
    "        x = self.act_fun(self.fc1(x.view(-1, 512)))\n",
    "        x = self.fc2(x)\n",
    "        return x, aux1, aux2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep_NOsharing_NOaux2(nn.Module):\n",
    "    def __init__(self, hidden, act_fun):\n",
    "        super(Deep_NOsharing_NOaux2, self).__init__()\n",
    "        self.act_fun = act_fun\n",
    "        self.conv1_1 = nn.Conv2d(1, 16, kernel_size=5)\n",
    "        self.conv1_2 = nn.Conv2d(1, 16, kernel_size=5)\n",
    "        self.conv2_1 = nn.Conv2d(16, 32, kernel_size=3)\n",
    "        self.conv2_2 = nn.Conv2d(16, 32, kernel_size=3)\n",
    "        self.conv3_1 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.conv3_2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        \n",
    "        self.conv4_1 = nn.Conv2d(64, 128, kernel_size=2)\n",
    "        self.conv4_2 = nn.Conv2d(64, 128, kernel_size=2)\n",
    "        \n",
    "        # After concatenation of the features from image 1 and image 2\n",
    "        self.fc1 = nn.Linear(1024, hidden)\n",
    "        self.fc2 = nn.Linear(hidden,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1_1 = self.act_fun(self.conv1_1(x[:,0,:,:].view(100,1,14,14)))\n",
    "        x2_1 = self.act_fun(self.conv2_1(x1_1))\n",
    "        x3_1 = self.act_fun(self.conv3_2(x2_1))\n",
    "        x4_1 = self.act_fun(F.max_pool2d(self.conv4_1(x3_1), kernel_size=2, stride=2))\n",
    "        \n",
    "        x1_2 = self.act_fun(self.conv1_2(x[:,1,:,:].view(100,1,14,14)))\n",
    "        x2_2 = self.act_fun(self.conv2_2(x1_2))\n",
    "        x3_2 = self.act_fun(self.conv3_2(x2_2))\n",
    "        x4_2 = self.act_fun(F.max_pool2d(self.conv4_2(x3_2), kernel_size=2, stride=2))\n",
    "\n",
    "        x = torch.cat([x4_1, x4_2],1)\n",
    "        x = self.act_fun(self.fc1(x.view(-1, 1024)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class Deep_sharing_NOaux2(nn.Module):\n",
    "    def __init__(self, hidden, act_fun):\n",
    "        super(Deep_sharing_NOaux2, self).__init__()\n",
    "        self.act_fun = act_fun\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=2)\n",
    "        \n",
    "        # After concatenation of the features from image 1 and image 2\n",
    "        self.fc1 = nn.Linear(1024, hidden)\n",
    "        self.fc2 = nn.Linear(hidden,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_images = []\n",
    "        for image in range(2):\n",
    "            x1 = self.act_fun(self.conv1(x[:,image,:,:].view(100,1,14,14)))\n",
    "            x2 = self.act_fun(self.conv2(x1))\n",
    "            x3 = self.act_fun(self.conv3(x2))\n",
    "            x4 = self.act_fun(F.max_pool2d(self.conv4(x3), kernel_size=2, stride=2))\n",
    "            conv_images.append(x4)\n",
    "        \n",
    "        x = torch.cat([conv_images[0], conv_images[1]],1)\n",
    "        x = self.act_fun(self.fc1(x.view(-1, 1024)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class Deep_NOsharing_aux2(nn.Module):\n",
    "    def __init__(self, hidden, act_fun):\n",
    "        super(Deep_NOsharing_aux2, self).__init__()\n",
    "        self.act_fun = act_fun\n",
    "        self.conv1_1 = nn.Conv2d(1, 16, kernel_size=5)\n",
    "        self.conv1_2 = nn.Conv2d(1, 16, kernel_size=5)\n",
    "        self.conv2_1 = nn.Conv2d(16, 32, kernel_size=3)\n",
    "        self.conv2_2 = nn.Conv2d(16, 32, kernel_size=3)\n",
    "        self.conv3_1 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.conv3_2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        \n",
    "        self.conv4_1 = nn.Conv2d(64, 128, kernel_size=2)\n",
    "        self.conv4_2 = nn.Conv2d(64, 128, kernel_size=2)\n",
    "        \n",
    "        # For classification with classes\n",
    "        self.fc_aux1 = nn.Linear(512, 10)\n",
    "        self.fc_aux2 = nn.Linear(512, 10)\n",
    "        \n",
    "        # After concatenation of the features from image 1 and image 2\n",
    "        self.fc1 = nn.Linear(1024, hidden)\n",
    "        self.fc2 = nn.Linear(hidden,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1_1 = self.act_fun(self.conv1_1(x[:,0,:,:].view(100,1,14,14)))\n",
    "        x2_1 = self.act_fun(self.conv2_1(x1_1))\n",
    "        x3_1 = self.act_fun(self.conv3_2(x2_1))\n",
    "        x4_1 = self.act_fun(F.max_pool2d(self.conv4_1(x3_1), kernel_size=2, stride=2))\n",
    "        \n",
    "        x1_2 = self.act_fun(self.conv1_2(x[:,1,:,:].view(100,1,14,14)))\n",
    "        x2_2 = self.act_fun(self.conv2_2(x1_2))\n",
    "        x3_2 = self.act_fun(self.conv3_2(x2_2))\n",
    "        x4_2 = self.act_fun(F.max_pool2d(self.conv4_2(x3_2), kernel_size=2, stride=2))\n",
    "\n",
    "        aux1 = F.softmax(self.fc_aux1(x4_1.view(-1,512)),1)\n",
    "        aux2 = F.softmax(self.fc_aux2(x4_2.view(-1,512)),1)\n",
    "        \n",
    "        x = torch.cat([x4_1, x4_2],1)\n",
    "        x = self.act_fun(self.fc1(x.view(-1, 1024)))\n",
    "        x = self.fc2(x)\n",
    "        return x, aux1, aux2\n",
    "    \n",
    "class Deep_sharing_aux2(nn.Module):\n",
    "    def __init__(self, hidden, act_fun):\n",
    "        super(Deep_sharing_aux2, self).__init__()\n",
    "        self.act_fun = act_fun\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=2)\n",
    "        \n",
    "        # For classification with classes\n",
    "        self.fc_aux1 = nn.Linear(512, 10)\n",
    "        self.fc_aux2 = nn.Linear(512, 10)\n",
    "        \n",
    "        # After concatenation of the features from image 1 and image 2\n",
    "        self.fc1 = nn.Linear(1024, hidden)\n",
    "        self.fc2 = nn.Linear(hidden,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_images = []\n",
    "        for image in range(2):\n",
    "            x1 = self.act_fun(self.conv1(x[:,image,:,:].view(100,1,14,14)))\n",
    "            x2 = self.act_fun(self.conv2(x1))\n",
    "            x3 = self.act_fun(self.conv3(x2))\n",
    "            x4 = self.act_fun(F.max_pool2d(self.conv4(x3), kernel_size=2, stride=2))\n",
    "            conv_images.append(x4)\n",
    "            \n",
    "        aux1 = F.softmax(self.fc_aux1(conv_images[0].view(-1,512)),1)\n",
    "        aux2 = F.softmax(self.fc_aux2(conv_images[1].view(-1,512)),1)\n",
    "        \n",
    "        x = torch.cat([conv_images[0], conv_images[1]],1)\n",
    "        x = self.act_fun(self.fc1(x.view(-1, 1024)))\n",
    "        x = self.fc2(x)\n",
    "        return x, aux1, aux2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch_size = 100\n",
    "nb_epochs = 25\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "eta = 0.001\n",
    "hidden_units = 300\n",
    "\n",
    "\n",
    "\n",
    "# Shallow\n",
    "model_shallow_NOsharing_NOaux = Shallow_NOsharing_NOaux(hidden = hidden_units, act_fun = F.relu)\n",
    "model_shallow_sharing_NOaux = Shallow_sharing_NOaux(hidden = hidden_units, act_fun = F.relu)\n",
    "model_shallow_NOsharing_aux = Shallow_NOsharing_aux(hidden = hidden_units, act_fun = F.relu)\n",
    "model_shallow_sharing_aux = Shallow_sharing_aux(hidden = hidden_units, act_fun = F.relu)\n",
    "\n",
    "#MLP\n",
    "model_MLP_NOsharing_NOaux = MLP_NOsharing_NOaux(hidden = hidden_units, act_fun = F.relu)\n",
    "model_MLP_sharing_NOaux = MLP_sharing_NOaux(hidden = hidden_units, act_fun = F.relu)\n",
    "model_MLP_NOsharing_aux = MLP_NOsharing_aux(hidden = hidden_units, act_fun = F.relu)\n",
    "model_MLP_sharing_aux = MLP_sharing_aux(hidden = hidden_units, act_fun = F.relu)\n",
    "\n",
    "#Deep1\n",
    "model_deep_NOsharing_NOaux = Deep_NOsharing_NOaux(hidden = hidden_units, act_fun = F.relu)\n",
    "model_deep_sharing_NOaux = Deep_sharing_NOaux(hidden = hidden_units, act_fun = F.relu)\n",
    "model_deep_NOsharing_aux = Deep_NOsharing_aux(hidden = hidden_units, act_fun = F.relu)\n",
    "model_deep_sharing_aux = Deep_sharing_aux(hidden = hidden_units, act_fun = F.relu)\n",
    "\n",
    "#Deep2\n",
    "model_deep_NOsharing_NOaux2 = Deep_NOsharing_NOaux2(hidden = hidden_units, act_fun = F.relu)\n",
    "model_deep_sharing_NOaux2 = Deep_sharing_NOaux2(hidden = hidden_units, act_fun = F.relu)\n",
    "model_deep_NOsharing_aux2 = Deep_NOsharing_aux2(hidden = hidden_units, act_fun = F.relu)\n",
    "model_deep_sharing_aux2 = Deep_sharing_aux2(hidden = hidden_units, act_fun = F.relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shallow\n",
    "train_model_NOaux(model_shallow_NOsharing_NOaux, train_input, train_target,nb_epochs, mini_batch_size, criterion, eta)\n",
    "train_model_NOaux(model_shallow_sharing_NOaux, train_input, train_target,nb_epochs, mini_batch_size, criterion, eta)\n",
    "train_model_aux(model_shallow_NOsharing_aux, train_input, train_target,nb_epochs, mini_batch_size, criterion, eta, 0.5)\n",
    "train_model_aux(model_shallow_sharing_aux, train_input, train_target,nb_epochs, mini_batch_size, criterion, eta, 0.5)\n",
    "\n",
    "#MLP\n",
    "train_model_NOaux(model_MLP_NOsharing_NOaux, train_input, train_target,nb_epochs, mini_batch_size, criterion, eta)\n",
    "train_model_NOaux(model_MLP_sharing_NOaux, train_input, train_target,nb_epochs, mini_batch_size, criterion, eta)\n",
    "train_model_aux(model_MLP_NOsharing_aux, train_input, train_target,nb_epochs, mini_batch_size, criterion, eta, 0.5)\n",
    "train_model_aux(model_MLP_sharing_aux, train_input, train_target,nb_epochs, mini_batch_size, criterion, eta, 0.5)\n",
    "\n",
    "#Deep1\n",
    "train_model_NOaux(model_deep_sharing_NOaux, train_input, train_target,nb_epochs, mini_batch_size, criterion, eta)\n",
    "train_model_NOaux(model_deep_NOsharing_NOaux, train_input, train_target,nb_epochs, mini_batch_size, criterion, eta)\n",
    "train_model_aux(model_deep_NOsharing_aux, train_input, train_target,nb_epochs, mini_batch_size, criterion, eta, 0.5)\n",
    "train_model_aux(model_deep_sharing_aux, train_input, train_target,nb_epochs, mini_batch_size, criterion, eta, 0.5)\n",
    "\n",
    "#Deep2\n",
    "train_model_NOaux(model_deep_sharing_NOaux2, train_input, train_target,nb_epochs, mini_batch_size, criterion, eta)\n",
    "train_model_NOaux(model_deep_NOsharing_NOaux2, train_input, train_target,nb_epochs, mini_batch_size, criterion, eta)\n",
    "train_model_aux(model_deep_NOsharing_aux2, train_input, train_target,nb_epochs, mini_batch_size, criterion, eta, 0.5)\n",
    "train_model_aux(model_deep_sharing_aux2, train_input, train_target,nb_epochs, mini_batch_size, criterion, eta, 0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train performance shallow NOsharing NOaux =  1.0\n",
      "Train performance shallow sharing Noaux =  1.0\n",
      "Train performance shallow NOsharing aux =  1.0\n",
      "Train performance shallow sharing aux =  0.999\n",
      "Train performance MLP NOsharing NOaux =  1.0\n",
      "Train performance MLP sharing Noaux =  1.0\n",
      "Train performance MLP NOsharing aux =  1.0\n",
      "Train performance MLP sharing aux =  1.0\n",
      "Train performance Deep NOsharing NOaux =  0.979\n",
      "Train performance Deep sharing Noaux =  0.998\n",
      "Train performance Deep NOsharing aux =  0.995\n",
      "Train performance Deep sharing aux =  0.999\n",
      "Train performance Deep NOsharing NOaux 2 =  0.999\n",
      "Train performance Deep sharing Noaux 2 =  0.999\n",
      "Train performance Deep NOsharing aux 2 =  0.999\n",
      "Train performance Deep sharing aux 2 =  0.999\n",
      "\n",
      "Test performance shallow NOsharing NOaux =  0.7969999999999999\n",
      "Test performance shallow sharing Noaux =  0.802\n",
      "Test performance shallow NOsharing aux =  0.7989999999999999\n",
      "Test performance shallow sharing aux =  0.808\n",
      "Test performance MLP NOsharing NOaux =  0.83\n",
      "Test performance MLP sharing Noaux =  0.821\n",
      "Test performance MLP NOsharing aux =  0.8200000000000001\n",
      "Test performance MLP sharing aux =  0.8200000000000001\n",
      "Test performance Deep NOsharing NOaux =  0.817\n",
      "Test performance Deep sharing Noaux =  0.823\n",
      "Test performance Deep NOsharing aux =  0.84\n",
      "Test performance Deep sharing aux =  0.841\n",
      "Test performance Deep NOsharing NOaux 2 =  0.823\n",
      "Test performance Deep sharing Noaux 2 =  0.834\n",
      "Test performance Deep NOsharing aux 2 =  0.829\n",
      "Test performance Deep sharing aux 2 =  0.863\n"
     ]
    }
   ],
   "source": [
    "#Shallow\n",
    "train_acc_shallow_NOsharing_NOaux = 1 - compute_nb_errors_NOaux(model_shallow_NOsharing_NOaux, train_input, train_target, mini_batch_size)\n",
    "train_acc_shallow_sharing_NOaux  = 1 - compute_nb_errors_NOaux(model_shallow_sharing_NOaux, train_input, train_target, mini_batch_size)\n",
    "train_acc_shallow_NOsharing_aux  = 1 - compute_nb_errors_aux(model_shallow_NOsharing_aux, train_input, train_target, mini_batch_size)\n",
    "train_acc_shallow_sharing_aux  = 1 - compute_nb_errors_aux(model_shallow_sharing_aux, train_input, train_target, mini_batch_size)\n",
    "\n",
    "#MLP\n",
    "train_acc_MLP_NOsharing_NOaux = 1 - compute_nb_errors_NOaux(model_MLP_NOsharing_NOaux, train_input, train_target, mini_batch_size)\n",
    "train_acc_MLP_sharing_NOaux = 1 - compute_nb_errors_NOaux(model_MLP_sharing_NOaux, train_input, train_target, mini_batch_size)\n",
    "train_acc_MLP_NOsharing_aux = 1 - compute_nb_errors_aux(model_MLP_NOsharing_aux, train_input, train_target, mini_batch_size)\n",
    "train_acc_MLP_sharing_aux = 1 - compute_nb_errors_aux(model_MLP_sharing_aux, train_input, train_target, mini_batch_size)\n",
    "\n",
    "#Deep1\n",
    "train_acc_deep_NOsharing_NOaux = 1 - compute_nb_errors_NOaux(model_deep_NOsharing_NOaux, train_input, train_target, mini_batch_size)\n",
    "train_acc_deep_sharing_NOaux = 1 - compute_nb_errors_NOaux(model_deep_sharing_NOaux, train_input, train_target, mini_batch_size)\n",
    "train_acc_deep_NOsharing_aux = 1 - compute_nb_errors_aux(model_deep_NOsharing_aux, train_input, train_target, mini_batch_size)\n",
    "train_acc_deep_sharing_aux = 1 - compute_nb_errors_aux(model_deep_sharing_aux, train_input, train_target, mini_batch_size)\n",
    "\n",
    "#Deep2\n",
    "train_acc_deep_NOsharing_NOaux2 = 1 - compute_nb_errors_NOaux(model_deep_NOsharing_NOaux2, train_input, train_target, mini_batch_size)\n",
    "train_acc_deep_sharing_NOaux2 = 1 - compute_nb_errors_NOaux(model_deep_sharing_NOaux2, train_input, train_target, mini_batch_size)\n",
    "train_acc_deep_NOsharing_aux2 = 1 - compute_nb_errors_aux(model_deep_NOsharing_aux2, train_input, train_target, mini_batch_size)\n",
    "train_acc_deep_sharing_aux2 = 1 - compute_nb_errors_aux(model_deep_sharing_aux2, train_input, train_target, mini_batch_size)\n",
    "\n",
    "print('Train performance shallow NOsharing NOaux = ', train_acc_shallow_NOsharing_NOaux)\n",
    "print('Train performance shallow sharing Noaux = ', train_acc_shallow_sharing_NOaux)\n",
    "print('Train performance shallow NOsharing aux = ', train_acc_shallow_NOsharing_aux)\n",
    "print('Train performance shallow sharing aux = ', train_acc_shallow_sharing_aux)\n",
    "\n",
    "print('Train performance MLP NOsharing NOaux = ', train_acc_MLP_NOsharing_NOaux)\n",
    "print('Train performance MLP sharing Noaux = ', train_acc_MLP_sharing_NOaux)\n",
    "print('Train performance MLP NOsharing aux = ', train_acc_MLP_NOsharing_aux)\n",
    "print('Train performance MLP sharing aux = ', train_acc_MLP_sharing_aux)\n",
    "\n",
    "print('Train performance Deep NOsharing NOaux = ', train_acc_deep_NOsharing_NOaux)\n",
    "print('Train performance Deep sharing Noaux = ', train_acc_deep_sharing_NOaux)\n",
    "print('Train performance Deep NOsharing aux = ', train_acc_deep_NOsharing_aux)\n",
    "print('Train performance Deep sharing aux = ', train_acc_deep_sharing_aux)\n",
    "\n",
    "print('Train performance Deep NOsharing NOaux 2 = ', train_acc_deep_NOsharing_NOaux2)\n",
    "print('Train performance Deep sharing Noaux 2 = ', train_acc_deep_sharing_NOaux2)\n",
    "print('Train performance Deep NOsharing aux 2 = ', train_acc_deep_NOsharing_aux2)\n",
    "print('Train performance Deep sharing aux 2 = ', train_acc_deep_sharing_aux2)\n",
    "\n",
    "#Shallow\n",
    "test_acc_shallow_NOsharing_NOaux = 1 - compute_nb_errors_NOaux(model_shallow_NOsharing_NOaux, test_input, test_target, mini_batch_size)\n",
    "test_acc_shallow_sharing_NOaux = 1 - compute_nb_errors_NOaux(model_shallow_sharing_NOaux, test_input, test_target, mini_batch_size)\n",
    "test_acc_shallow_NOsharing_aux = 1 - compute_nb_errors_aux(model_shallow_NOsharing_aux, test_input, test_target, mini_batch_size)\n",
    "test_acc_shallow_sharing_aux = 1 - compute_nb_errors_aux(model_shallow_sharing_aux, test_input, test_target, mini_batch_size)\n",
    "\n",
    "#MLP\n",
    "test_acc_MLP_NOsharing_NOaux = 1 - compute_nb_errors_NOaux(model_MLP_NOsharing_NOaux, test_input, test_target, mini_batch_size)\n",
    "test_acc_MLP_sharing_NOaux = 1 - compute_nb_errors_NOaux(model_MLP_sharing_NOaux, test_input, test_target, mini_batch_size)\n",
    "test_acc_MLP_NOsharing_aux = 1 - compute_nb_errors_aux(model_MLP_NOsharing_aux, test_input, test_target, mini_batch_size)\n",
    "test_acc_MLP_sharing_aux = 1 - compute_nb_errors_aux(model_MLP_sharing_aux, test_input, test_target, mini_batch_size)\n",
    "\n",
    "#Deep1\n",
    "test_acc_deep_NOsharing_NOaux = 1 - compute_nb_errors_NOaux(model_deep_NOsharing_NOaux, test_input, test_target, mini_batch_size)\n",
    "test_acc_deep_sharing_NOaux = 1 - compute_nb_errors_NOaux(model_deep_sharing_NOaux, test_input, test_target, mini_batch_size)\n",
    "test_acc_deep_NOsharing_aux = 1 - compute_nb_errors_aux(model_deep_NOsharing_aux, test_input, test_target, mini_batch_size)\n",
    "test_acc_deep_sharing_aux = 1 - compute_nb_errors_aux(model_deep_sharing_aux, test_input, test_target, mini_batch_size)\n",
    "\n",
    "#Deep2\n",
    "test_acc_deep_NOsharing_NOaux2 = 1 - compute_nb_errors_NOaux(model_deep_NOsharing_NOaux2, test_input, test_target, mini_batch_size)\n",
    "test_acc_deep_sharing_NOaux2 = 1 - compute_nb_errors_NOaux(model_deep_sharing_NOaux2, test_input, test_target, mini_batch_size)\n",
    "test_acc_deep_NOsharing_aux2 = 1 - compute_nb_errors_aux(model_deep_NOsharing_aux2, test_input, test_target, mini_batch_size)\n",
    "test_acc_deep_sharing_aux2 = 1 - compute_nb_errors_aux(model_deep_sharing_aux2, test_input, test_target, mini_batch_size)\n",
    "\n",
    "print('')\n",
    "print('Test performance shallow NOsharing NOaux = ', test_acc_shallow_NOsharing_NOaux)\n",
    "print('Test performance shallow sharing Noaux = ', test_acc_shallow_sharing_NOaux)\n",
    "print('Test performance shallow NOsharing aux = ', test_acc_shallow_NOsharing_aux)\n",
    "print('Test performance shallow sharing aux = ', test_acc_shallow_sharing_aux)\n",
    "\n",
    "print('Test performance MLP NOsharing NOaux = ', test_acc_MLP_NOsharing_NOaux)\n",
    "print('Test performance MLP sharing Noaux = ', test_acc_MLP_sharing_NOaux)\n",
    "print('Test performance MLP NOsharing aux = ', test_acc_MLP_NOsharing_aux)\n",
    "print('Test performance MLP sharing aux = ', test_acc_MLP_sharing_aux)\n",
    "\n",
    "print('Test performance Deep NOsharing NOaux = ', test_acc_deep_NOsharing_NOaux)\n",
    "print('Test performance Deep sharing Noaux = ', test_acc_deep_sharing_NOaux)\n",
    "print('Test performance Deep NOsharing aux = ', test_acc_deep_NOsharing_aux)\n",
    "print('Test performance Deep sharing aux = ', test_acc_deep_sharing_aux)\n",
    "\n",
    "print('Test performance Deep NOsharing NOaux 2 = ', test_acc_deep_NOsharing_NOaux2)\n",
    "print('Test performance Deep sharing Noaux 2 = ', test_acc_deep_sharing_NOaux2)\n",
    "print('Test performance Deep NOsharing aux 2 = ', test_acc_deep_NOsharing_aux2)\n",
    "print('Test performance Deep sharing aux 2 = ', test_acc_deep_sharing_aux2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
