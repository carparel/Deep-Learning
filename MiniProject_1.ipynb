{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import dlc_practical_prologue as prologue\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_input size = torch.Size([1000, 2, 14, 14])\n",
      "train_target size = torch.Size([1000])\n",
      "train_classes size = torch.Size([1000, 2])\n",
      "test_input size = torch.Size([1000, 2, 14, 14])\n",
      "test_target size = torch.Size([1000])\n",
      "test_classes size = torch.Size([1000, 2])\n"
     ]
    }
   ],
   "source": [
    "nbr_pairs = 1000\n",
    "train_input, train_target, train_classes, test_input, test_target, test_classes = prologue.generate_pair_sets(nbr_pairs)\n",
    "\n",
    "print('train_input size =', train_input.size())\n",
    "print('train_target size =', train_target.size()) #The boolean telling if the two pairs are the same or not \n",
    "print('train_classes size =', train_classes.size())\n",
    "print('test_input size =', test_input.size())\n",
    "print('test_target size =', test_target.size())\n",
    "print('test_classes size =', test_classes.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_NOaux(model, train_input, train_target, nb_epochs, batch_size, criterion, eta): \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = eta)\n",
    "    for e in range(nb_epochs):\n",
    "        if (e % 10 == 0 and e > 0):\n",
    "            eta = eta/10\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr = eta)\n",
    "        for step_ in range(0,train_input.size(0),batch_size):                              \n",
    "            output = model(train_input[step_:step_+batch_size])\n",
    "            loss = criterion(output, train_target[step_:step_+batch_size])\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def train_model_aux(model, train_input, train_target, nb_epochs, batch_size, criterion, eta, lambda_):   \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = eta)\n",
    "    for e in range(nb_epochs):\n",
    "        if (e % 10 == 0 and e > 0):\n",
    "            eta = eta/10\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr = eta)\n",
    "        for step_ in range(0,train_input.size(0),batch_size):                              \n",
    "            output_target, output_im1, output_im2 = model(train_input[step_:step_+batch_size])\n",
    "            loss_target = criterion(output_target, train_target[step_:step_+batch_size])\n",
    "            loss_im1 = criterion(output_im1, train_classes[step_:step_+batch_size,0])\n",
    "            loss_im2 = criterion(output_im2, train_classes[step_:step_+batch_size,1])\n",
    "            loss = loss_target + lambda_*(loss_im1 + loss_im2)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors_NOaux(model, data_input, data_target, mini_batch_size): \n",
    "    nb_errors = 0\n",
    "    for b in range(0, data_input.size(0), mini_batch_size):\n",
    "        output = model(data_input.narrow(0, b, mini_batch_size))\n",
    "        _, predicted_classes = output.data.max(1)\n",
    "        for k in range(mini_batch_size):\n",
    "            if data_target[b + k] != predicted_classes[k]:\n",
    "                nb_errors = nb_errors + 1\n",
    "    return nb_errors\n",
    "\n",
    "def compute_nb_errors_aux(model, data_input, data_target, mini_batch_size): \n",
    "    nb_errors = 0\n",
    "    for b in range(0, data_input.size(0), mini_batch_size):\n",
    "        output,_,_ = model(data_input.narrow(0, b, mini_batch_size))\n",
    "        _, predicted_classes = output.data.max(1)\n",
    "        for k in range(mini_batch_size):\n",
    "            if data_target[b + k] != predicted_classes[k]:\n",
    "                nb_errors = nb_errors + 1\n",
    "    return nb_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shallow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Shallow_NOsharing_NOaux(nn.Module):\n",
    "    def __init__(self, hidden, act_fun):\n",
    "        super(Shallow_NOsharing_NOaux, self).__init__()\n",
    "        self.act_fun = act_fun\n",
    "        self.fc1_1 = nn.Linear(196, hidden)\n",
    "        self.fc1_2 = nn.Linear(196, hidden)\n",
    "        # After concatenation of the features from image 1 and image 2\n",
    "        self.fc2 = nn.Linear(hidden*2,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_1 = self.act_fun(self.fc1_1(x[:,0,:,:].view(-1,196)))\n",
    "        x_2 = self.act_fun(self.fc1_2(x[:,1,:,:].view(-1,196)))\n",
    "        x = torch.cat([x_1, x_2],1)\n",
    "        x = self.fc2(x)       \n",
    "        return x\n",
    "    \n",
    "class Shallow_sharing_NOaux(nn.Module):\n",
    "    def __init__(self, hidden, act_fun):\n",
    "        super(Shallow_sharing_NOaux, self).__init__()\n",
    "        self.act_fun = act_fun\n",
    "        self.fc1 = nn.Linear(196, hidden)\n",
    "        # After concatenation of the features from image 1 and image 2\n",
    "        self.fc2 = nn.Linear(hidden*2,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        fc_image = []\n",
    "        for image in range(2):\n",
    "            x1 = self.act_fun(self.fc1(x[:,image,:,:].view(-1,196)))\n",
    "            fc_image.append(x1)\n",
    "        x = torch.cat([fc_image[0],fc_image[1]],1)\n",
    "        x = self.fc2(x)       \n",
    "        return x\n",
    "    \n",
    "class Shallow_NOsharing_aux(nn.Module):\n",
    "    def __init__(self, hidden, act_fun):\n",
    "        super(Shallow_NOsharing_aux, self).__init__()\n",
    "        self.act_fun = act_fun\n",
    "        self.fc1_1 = nn.Linear(196, hidden)\n",
    "        self.fc1_2 = nn.Linear(196, hidden)\n",
    "        \n",
    "        # For classification with classes\n",
    "        self.fc_aux1 = nn.Linear(hidden, 10)\n",
    "        self.fc_aux2 = nn.Linear(hidden, 10)\n",
    "        \n",
    "        # After concatenation of the features from image 1 and image 2\n",
    "        self.fc2 = nn.Linear(hidden*2,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.act_fun(self.fc1_1(x[:,0,:,:].view(-1,196)))\n",
    "        x2 = self.act_fun(self.fc1_2(x[:,1,:,:].view(-1,196)))\n",
    "        \n",
    "        aux1 = F.softmax(self.fc_aux1(x1),1)\n",
    "        aux2 = F.softmax(self.fc_aux2(x2),1)\n",
    "        \n",
    "        x = torch.cat([x1, x2],1)\n",
    "        x = self.fc2(x)       \n",
    "        return x, aux1, aux2\n",
    "    \n",
    "class Shallow_sharing_aux(nn.Module):\n",
    "    def __init__(self, hidden, act_fun):\n",
    "        super(Shallow_sharing_aux, self).__init__()\n",
    "        self.act_fun = act_fun\n",
    "        self.fc1 = nn.Linear(196, hidden)\n",
    "        \n",
    "        # For classification with classes\n",
    "        self.fc_aux1 = nn.Linear(hidden, 10)\n",
    "        self.fc_aux2 = nn.Linear(hidden, 10)\n",
    "        \n",
    "        # After concatenation of the features from image 1 and image 2\n",
    "        self.fc2 = nn.Linear(hidden*2,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        fc_image = []\n",
    "        for image in range(2):\n",
    "            x1 = self.act_fun(self.fc1(x[:,image,:,:].view(-1,196)))\n",
    "            fc_image.append(x1)\n",
    "        \n",
    "        aux1 = F.softmax(self.fc_aux1(fc_image[0]),1)\n",
    "        aux2 = F.softmax(self.fc_aux2(fc_image[1]),1)\n",
    "        \n",
    "        x = torch.cat([fc_image[0],fc_image[1]],1)\n",
    "        x = self.fc2(x)       \n",
    "        return x, aux1, aux2      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_NOsharing_NOaux(nn.Module):\n",
    "    def __init__(self, hidden, act_fun):\n",
    "        super(MLP_NOsharing_NOaux, self).__init__()\n",
    "        self.act_fun = act_fun\n",
    "        self.fc1_1 = nn.Linear(196, hidden)\n",
    "        self.fc1_2 = nn.Linear(196, hidden)\n",
    "        self.fc2_1 = nn.Linear(hidden,hidden)\n",
    "        self.fc2_2 = nn.Linear(hidden,hidden)\n",
    "        # After concatenation of the features from image 1 and image 2\n",
    "        self.fc3 = nn.Linear(hidden*2,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1_1 = self.act_fun(self.fc1_1(x[:,0,:,:].view(-1,196)))\n",
    "        x1_2 = self.act_fun(self.fc1_2(x[:,1,:,:].view(-1,196)))\n",
    "        x2_1 = self.act_fun(self.fc2_1(x1_1))\n",
    "        x2_2 = self.act_fun(self.fc2_2(x1_2))\n",
    "        x = torch.cat([x2_1, x2_2],1)\n",
    "        x = self.fc3(x)       \n",
    "        return x\n",
    "\n",
    "class MLP_sharing_NOaux(nn.Module):\n",
    "    def __init__(self, hidden, act_fun):\n",
    "        super(MLP_sharing_NOaux, self).__init__()\n",
    "        self.act_fun = act_fun\n",
    "        self.fc1 = nn.Linear(196, hidden)\n",
    "        self.fc2 = nn.Linear(hidden,hidden)\n",
    "        # After concatenation of the features from image 1 and image 2\n",
    "        self.fc3 = nn.Linear(hidden*2,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        fc_image = []\n",
    "        for image in range(2):\n",
    "            x1 = self.act_fun(self.fc1(x[:,image,:,:].view(-1,196)))\n",
    "            x2 = self.act_fun(self.fc2(x1))\n",
    "            fc_image.append(x2)\n",
    "        x = torch.cat([fc_image[0],fc_image[1]],1)\n",
    "        x = self.fc3(x)       \n",
    "        return x\n",
    "    \n",
    "class MLP_NOsharing_aux(nn.Module):\n",
    "    def __init__(self, hidden, act_fun):\n",
    "        super(MLP_NOsharing_aux, self).__init__()\n",
    "        self.act_fun = act_fun\n",
    "        self.fc1_1 = nn.Linear(196, hidden)\n",
    "        self.fc1_2 = nn.Linear(196, hidden)\n",
    "        self.fc2_1 = nn.Linear(hidden,hidden)\n",
    "        self.fc2_2 = nn.Linear(hidden,hidden)\n",
    "        \n",
    "        # For classification with classes\n",
    "        self.fc_aux1 = nn.Linear(hidden, 10)\n",
    "        self.fc_aux2 = nn.Linear(hidden, 10)\n",
    "        \n",
    "        # After concatenation of the features from image 1 and image 2\n",
    "        self.fc3 = nn.Linear(hidden*2,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1_1 = self.act_fun(self.fc1_1(x[:,0,:,:].view(-1,196)))\n",
    "        x1_2 = self.act_fun(self.fc1_2(x[:,1,:,:].view(-1,196)))\n",
    "        x2_1 = self.act_fun(self.fc2_1(x1_1))\n",
    "        x2_2 = self.act_fun(self.fc2_2(x1_2))\n",
    "        \n",
    "        aux1 = F.softmax(self.fc_aux1(x2_1),1)\n",
    "        aux2 = F.softmax(self.fc_aux2(x2_2),1)\n",
    "        \n",
    "        x = torch.cat([x2_1, x2_2],1)\n",
    "        x = self.fc3(x)       \n",
    "        return x, aux1, aux2\n",
    "    \n",
    "class MLP_sharing_aux(nn.Module):\n",
    "    def __init__(self, hidden, act_fun):\n",
    "        super(MLP_sharing_aux, self).__init__()\n",
    "        self.act_fun = act_fun\n",
    "        self.fc1 = nn.Linear(196, hidden)\n",
    "        self.fc2 = nn.Linear(hidden,hidden)\n",
    "        \n",
    "        # For classification with classes\n",
    "        self.fc_aux1 = nn.Linear(hidden, 10)\n",
    "        self.fc_aux2 = nn.Linear(hidden, 10)\n",
    "        \n",
    "        # After concatenation of the features from image 1 and image 2\n",
    "        self.fc3 = nn.Linear(hidden*2,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        fc_image = []\n",
    "        for image in range(2):\n",
    "            x1 = self.act_fun(self.fc1(x[:,image,:,:].view(-1,196)))\n",
    "            x2 = self.act_fun(self.fc2(x1))\n",
    "            fc_image.append(x2)\n",
    "            \n",
    "        aux1 = F.softmax(self.fc_aux1(fc_image[0]),1)\n",
    "        aux2 = F.softmax(self.fc_aux2(fc_image[1]),1)\n",
    "        \n",
    "        x = torch.cat([fc_image[0],fc_image[1]],1)\n",
    "        x = self.fc3(x)       \n",
    "        return x, aux1, aux2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep_NOsharing_NOaux(nn.Module):\n",
    "    def __init__(self, hidden, act_fun):\n",
    "        super(Deep_NOsharing_NOaux, self).__init__()\n",
    "        self.act_fun = act_fun\n",
    "        self.conv1_1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv1_2 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2_1 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.conv2_2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        \n",
    "        # After concatenation of the features from image 1 and image 2\n",
    "        self.fc1 = nn.Linear(512, hidden)\n",
    "        self.fc2 = nn.Linear(hidden,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1_1 = self.act_fun(F.max_pool2d(self.conv1_1(x[:,0,:,:].view(100,1,14,14)), kernel_size=2, stride=2))\n",
    "        x2_1 = self.act_fun(F.max_pool2d(self.conv2_1(x1_1), kernel_size=2, stride=2))\n",
    "        \n",
    "        x1_2 = self.act_fun(F.max_pool2d(self.conv1_2(x[:,1,:,:].view(100,1,14,14)), kernel_size=2, stride=2))\n",
    "        x2_2 = self.act_fun(F.max_pool2d(self.conv2_2(x1_2), kernel_size=2, stride=2))\n",
    "        \n",
    "        x = torch.cat([x2_1, x2_2],1)\n",
    "        x = self.act_fun(self.fc1(x.view(-1, 512)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class Deep_sharing_NOaux(nn.Module):\n",
    "    def __init__(self, hidden, act_fun):\n",
    "        super(Deep_sharing_NOaux, self).__init__()\n",
    "        self.act_fun = act_fun\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        \n",
    "        # After concatenation of the features from image 1 and image 2\n",
    "        self.fc1 = nn.Linear(512, hidden)\n",
    "        self.fc2 = nn.Linear(hidden,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_images = []\n",
    "        for image in range(2):\n",
    "            first_conv = self.act_fun(F.max_pool2d(self.conv1(x[:,image,:,:].view(100,1,14,14)), kernel_size=2, stride=2))\n",
    "            conv_images.append(self.act_fun(F.max_pool2d(self.conv2(first_conv), kernel_size=2, stride=2)))\n",
    "        \n",
    "        x = torch.cat([conv_images[0], conv_images[1]],1)\n",
    "        x = self.act_fun(self.fc1(x.view(-1, 512)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class Deep_NOsharing_aux(nn.Module):\n",
    "    def __init__(self, hidden, act_fun):\n",
    "        super(Deep_NOsharing_aux, self).__init__()\n",
    "        self.act_fun = act_fun\n",
    "        self.conv1_1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv1_2 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2_1 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.conv2_2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        \n",
    "        # For classification with classes\n",
    "        self.fc_aux1 = nn.Linear(256, 10)\n",
    "        self.fc_aux2 = nn.Linear(256, 10)\n",
    "        \n",
    "        # After concatenation of the features from image 1 and image 2\n",
    "        self.fc1 = nn.Linear(512, hidden)\n",
    "        self.fc2 = nn.Linear(hidden,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1_1 = self.act_fun(F.max_pool2d(self.conv1_1(x[:,0,:,:].view(100,1,14,14)), kernel_size=2, stride=2))\n",
    "        x2_1 = self.act_fun(F.max_pool2d(self.conv2_1(x1_1), kernel_size=2, stride=2))\n",
    "        \n",
    "        x1_2 = self.act_fun(F.max_pool2d(self.conv1_2(x[:,1,:,:].view(100,1,14,14)), kernel_size=2, stride=2))\n",
    "        x2_2 = self.act_fun(F.max_pool2d(self.conv2_2(x1_2), kernel_size=2, stride=2))\n",
    "\n",
    "        aux1 = F.softmax(self.fc_aux1(x2_1.view(-1,256)),1)\n",
    "        aux2 = F.softmax(self.fc_aux2(x2_2.view(-1,256)),1)\n",
    "        \n",
    "        x = torch.cat([x2_1, x2_2],1)\n",
    "        x = self.act_fun(self.fc1(x.view(-1, 512)))\n",
    "        x = self.fc2(x)\n",
    "        return x, aux1, aux2\n",
    "    \n",
    "class Deep_sharing_aux(nn.Module):\n",
    "    def __init__(self, hidden, act_fun):\n",
    "        super(Deep_sharing_aux, self).__init__()\n",
    "        self.act_fun = act_fun\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        \n",
    "        # For classification with classes\n",
    "        self.fc_aux1 = nn.Linear(256, 10)\n",
    "        self.fc_aux2 = nn.Linear(256, 10)\n",
    "        \n",
    "        # After concatenation of the features from image 1 and image 2\n",
    "        self.fc1 = nn.Linear(512, hidden)\n",
    "        self.fc2 = nn.Linear(hidden,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_images = []\n",
    "        for image in range(2):\n",
    "            first_conv = self.act_fun(F.max_pool2d(self.conv1(x[:,image,:,:].view(100,1,14,14)), kernel_size=2, stride=2))\n",
    "            conv_images.append(self.act_fun(F.max_pool2d(self.conv2(first_conv), kernel_size=2, stride=2)))\n",
    "            \n",
    "        aux1 = F.softmax(self.fc_aux1(conv_images[0].view(-1,256)),1)\n",
    "        aux2 = F.softmax(self.fc_aux2(conv_images[1].view(-1,256)),1)\n",
    "        \n",
    "        x = torch.cat([conv_images[0], conv_images[1]],1)\n",
    "        x = self.act_fun(self.fc1(x.view(-1, 512)))\n",
    "        x = self.fc2(x)\n",
    "        return x, aux1, aux2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep_NOsharing_NOaux2(nn.Module):\n",
    "    def __init__(self, hidden, act_fun):\n",
    "        super(Deep_NOsharing_NOaux2, self).__init__()\n",
    "        self.act_fun = act_fun\n",
    "        self.conv1_1 = nn.Conv2d(1, 16, kernel_size=5)\n",
    "        self.conv1_2 = nn.Conv2d(1, 16, kernel_size=5)\n",
    "        self.conv2_1 = nn.Conv2d(16, 32, kernel_size=3)\n",
    "        self.conv2_2 = nn.Conv2d(16, 32, kernel_size=3)\n",
    "        self.conv3_1 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.conv3_2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        \n",
    "        self.conv4_1 = nn.Conv2d(64, 128, kernel_size=2)\n",
    "        self.conv4_2 = nn.Conv2d(64, 128, kernel_size=2)\n",
    "        \n",
    "        # After concatenation of the features from image 1 and image 2\n",
    "        self.fc1 = nn.Linear(1024, hidden)\n",
    "        self.fc2 = nn.Linear(hidden,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1_1 = self.act_fun(self.conv1_1(x[:,0,:,:].view(100,1,14,14)))\n",
    "        x2_1 = self.act_fun(self.conv2_1(x1_1))\n",
    "        x3_1 = self.act_fun(self.conv3_2(x2_1))\n",
    "        x4_1 = self.act_fun(F.max_pool2d(self.conv4_1(x3_1), kernel_size=2, stride=2))\n",
    "        \n",
    "        x1_2 = self.act_fun(self.conv1_2(x[:,1,:,:].view(100,1,14,14)))\n",
    "        x2_2 = self.act_fun(self.conv2_2(x1_2))\n",
    "        x3_2 = self.act_fun(self.conv3_2(x2_2))\n",
    "        x4_2 = self.act_fun(F.max_pool2d(self.conv4_2(x3_2), kernel_size=2, stride=2))\n",
    "\n",
    "        x = torch.cat([x4_1, x4_2],1)\n",
    "        x = self.act_fun(self.fc1(x.view(-1, 1024)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class Deep_sharing_NOaux2(nn.Module):\n",
    "    def __init__(self, hidden, act_fun):\n",
    "        super(Deep_sharing_NOaux2, self).__init__()\n",
    "        self.act_fun = act_fun\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=2)\n",
    "        \n",
    "        # After concatenation of the features from image 1 and image 2\n",
    "        self.fc1 = nn.Linear(1024, hidden)\n",
    "        self.fc2 = nn.Linear(hidden,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_images = []\n",
    "        for image in range(2):\n",
    "            x1 = self.act_fun(self.conv1(x[:,image,:,:].view(100,1,14,14)))\n",
    "            x2 = self.act_fun(self.conv2(x1))\n",
    "            x3 = self.act_fun(self.conv3(x2))\n",
    "            x4 = self.act_fun(F.max_pool2d(self.conv4(x3), kernel_size=2, stride=2))\n",
    "            conv_images.append(x4)\n",
    "        \n",
    "        x = torch.cat([conv_images[0], conv_images[1]],1)\n",
    "        x = self.act_fun(self.fc1(x.view(-1, 1024)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class Deep_NOsharing_aux2(nn.Module):\n",
    "    def __init__(self, hidden, act_fun):\n",
    "        super(Deep_NOsharing_aux2, self).__init__()\n",
    "        self.act_fun = act_fun\n",
    "        self.conv1_1 = nn.Conv2d(1, 16, kernel_size=5)\n",
    "        self.conv1_2 = nn.Conv2d(1, 16, kernel_size=5)\n",
    "        self.conv2_1 = nn.Conv2d(16, 32, kernel_size=3)\n",
    "        self.conv2_2 = nn.Conv2d(16, 32, kernel_size=3)\n",
    "        self.conv3_1 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.conv3_2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        \n",
    "        self.conv4_1 = nn.Conv2d(64, 128, kernel_size=2)\n",
    "        self.conv4_2 = nn.Conv2d(64, 128, kernel_size=2)\n",
    "        \n",
    "        # For classification with classes\n",
    "        self.fc_aux1 = nn.Linear(512, 10)\n",
    "        self.fc_aux2 = nn.Linear(512, 10)\n",
    "        \n",
    "        # After concatenation of the features from image 1 and image 2\n",
    "        self.fc1 = nn.Linear(1024, hidden)\n",
    "        self.fc2 = nn.Linear(hidden,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1_1 = self.act_fun(self.conv1_1(x[:,0,:,:].view(100,1,14,14)))\n",
    "        x2_1 = self.act_fun(self.conv2_1(x1_1))\n",
    "        x3_1 = self.act_fun(self.conv3_2(x2_1))\n",
    "        x4_1 = self.act_fun(F.max_pool2d(self.conv4_1(x3_1), kernel_size=2, stride=2))\n",
    "        \n",
    "        x1_2 = self.act_fun(self.conv1_2(x[:,1,:,:].view(100,1,14,14)))\n",
    "        x2_2 = self.act_fun(self.conv2_2(x1_2))\n",
    "        x3_2 = self.act_fun(self.conv3_2(x2_2))\n",
    "        x4_2 = self.act_fun(F.max_pool2d(self.conv4_2(x3_2), kernel_size=2, stride=2))\n",
    "\n",
    "        aux1 = F.softmax(self.fc_aux1(x4_1.view(-1,512)),1)\n",
    "        aux2 = F.softmax(self.fc_aux2(x4_2.view(-1,512)),1)\n",
    "        \n",
    "        x = torch.cat([x4_1, x4_2],1)\n",
    "        x = self.act_fun(self.fc1(x.view(-1, 1024)))\n",
    "        x = self.fc2(x)\n",
    "        return x, aux1, aux2\n",
    "    \n",
    "class Deep_sharing_aux2(nn.Module):\n",
    "    def __init__(self, hidden, act_fun):\n",
    "        super(Deep_sharing_aux2, self).__init__()\n",
    "        self.act_fun = act_fun\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=2)\n",
    "        \n",
    "        # For classification with classes\n",
    "        self.fc_aux1 = nn.Linear(512, 10)\n",
    "        self.fc_aux2 = nn.Linear(512, 10)\n",
    "        \n",
    "        # After concatenation of the features from image 1 and image 2\n",
    "        self.fc1 = nn.Linear(1024, hidden)\n",
    "        self.fc2 = nn.Linear(hidden,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_images = []\n",
    "        for image in range(2):\n",
    "            x1 = self.act_fun(self.conv1(x[:,image,:,:].view(100,1,14,14)))\n",
    "            x2 = self.act_fun(self.conv2(x1))\n",
    "            x3 = self.act_fun(self.conv3(x2))\n",
    "            x4 = self.act_fun(F.max_pool2d(self.conv4(x3), kernel_size=2, stride=2))\n",
    "            conv_images.append(x4)\n",
    "            \n",
    "        aux1 = F.softmax(self.fc_aux1(conv_images[0].view(-1,512)),1)\n",
    "        aux2 = F.softmax(self.fc_aux2(conv_images[1].view(-1,512)),1)\n",
    "        \n",
    "        x = torch.cat([conv_images[0], conv_images[1]],1)\n",
    "        x = self.act_fun(self.fc1(x.view(-1, 1024)))\n",
    "        x = self.fc2(x)\n",
    "        return x, aux1, aux2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(input_, target, hidden_units, eta, lambda_, model_type = 'Shallow', sharing = False, \n",
    "                aux = False,  nb_epochs = 25, mini_batch_size = 100, criterion = nn.CrossEntropyLoss()):\n",
    "    if(model_type == 'Shallow'):\n",
    "        if(sharing):\n",
    "            if(aux): model = Shallow_sharing_aux(hidden = hidden_units, act_fun = F.relu)\n",
    "            else: model = Shallow_sharing_NOaux(hidden = hidden_units, act_fun = F.relu)\n",
    "        else:\n",
    "            if(aux): model = Shallow_NOsharing_aux(hidden = hidden_units, act_fun = F.relu)\n",
    "            else: model = Shallow_NOsharing_NOaux(hidden = hidden_units, act_fun = F.relu)\n",
    "    elif(model_type == 'MLP'):\n",
    "        if(sharing):\n",
    "            if(aux): model = MLP_sharing_aux(hidden = hidden_units, act_fun = F.relu)\n",
    "            else: model = MLP_sharing_NOaux(hidden = hidden_units, act_fun = F.relu)\n",
    "        else:\n",
    "            if(aux): model = MLP_NOsharing_aux(hidden = hidden_units, act_fun = F.relu)\n",
    "            else: model = MLP_NOsharing_NOaux(hidden = hidden_units, act_fun = F.relu)\n",
    "    elif(model_type == 'Deep1'):\n",
    "        if(sharing):\n",
    "            if(aux): model = Deep_sharing_aux(hidden = hidden_units, act_fun = F.relu)\n",
    "            else: model = Deep_sharing_NOaux(hidden = hidden_units, act_fun = F.relu)\n",
    "        else:\n",
    "            if(aux): model = Deep_NOsharing_aux(hidden = hidden_units, act_fun = F.relu)\n",
    "            else: model = Deep_NOsharing_NOaux(hidden = hidden_units, act_fun = F.relu)\n",
    "    elif(model_type == 'Deep2'):\n",
    "        if(sharing):\n",
    "            if(aux): model = Deep_sharing_aux2(hidden = hidden_units, act_fun = F.relu)\n",
    "            else: model = Deep_sharing_NOaux2(hidden = hidden_units, act_fun = F.relu)\n",
    "        else:\n",
    "            if(aux): model = Deep_NOsharing_aux2(hidden = hidden_units, act_fun = F.relu)\n",
    "            else: model = Deep_NOsharing_NOaux2(hidden = hidden_units, act_fun = F.relu)\n",
    "                \n",
    "    if(aux): \n",
    "        train_model_aux(model, input_[:700], target[:700], nb_epochs, mini_batch_size, criterion, eta, lambda_)\n",
    "        accuracy = 1 - compute_nb_errors_aux(model, input_[700:], target[700:], mini_batch_size)/len(target[700:])\n",
    "    else: \n",
    "        train_model_NOaux(model, input_[:700], target[:700], nb_epochs, mini_batch_size, criterion, eta)\n",
    "        accuracy = 1 - compute_nb_errors_NOaux(model, input_[700:], target[700:], mini_batch_size)/len(target[700:])\n",
    "    \n",
    "    return accuracy, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict():\n",
    "    results = {'Shallow':{'NOsharing_NOaux':{'Acc': 1, 'eta': 1, 'hidden': 1},\n",
    "                        'sharing_NOaux':{'Acc': 1, 'eta': 1, 'hidden': 1},\n",
    "                        'NOsharing_aux':{'Acc': 1, 'eta': 1, 'hidden': 1, 'lambda': 1},\n",
    "                        'sharing_aux':{'Acc': 1, 'eta': 1, 'hidden': 1, 'lambda': 1}},\n",
    "               'MLP':{'NOsharing_NOaux':{'Acc': 1, 'eta': 1, 'hidden': 1},\n",
    "                        'sharing_NOaux':{'Acc': 1, 'eta': 1, 'hidden': 1},\n",
    "                        'NOsharing_aux':{'Acc': 1, 'eta': 1, 'hidden': 1, 'lambda': 1},\n",
    "                        'sharing_aux':{'Acc': 1, 'eta': 1, 'hidden': 1, 'lambda': 1}},  \n",
    "               'Deep1':{'NOsharing_NOaux':{'Acc': 1, 'eta': 1, 'hidden': 1},\n",
    "                        'sharing_NOaux':{'Acc': 1, 'eta': 1, 'hidden': 1},\n",
    "                        'NOsharing_aux':{'Acc': 1, 'eta': 1, 'hidden': 1, 'lambda': 1},\n",
    "                        'sharing_aux':{'Acc': 1, 'eta': 1, 'hidden': 1, 'lambda': 1}},\n",
    "               'Deep2':{'NOsharing_NOaux':{'Acc': 1, 'eta': 1, 'hidden': 1},\n",
    "                        'sharing_NOaux':{'Acc': 1, 'eta': 1, 'hidden': 1},\n",
    "                        'NOsharing_aux':{'Acc': 1, 'eta': 1, 'hidden': 1, 'lambda': 1},\n",
    "                        'sharing_aux':{'Acc': 1, 'eta': 1, 'hidden': 1, 'lambda': 1}}  \n",
    "              }\n",
    "    return results\n",
    "\n",
    "def fill_results(results, type_model, sharing_flag, aux_flag, acc, eta, hidden, lambda_):\n",
    "    if(sharing_flag):\n",
    "        if(aux_flag):\n",
    "            results[type_model]['sharing_aux']['Acc'] = acc\n",
    "            results[type_model]['sharing_aux']['eta'] = eta\n",
    "            results[type_model]['sharing_aux']['hidden'] = hidden\n",
    "            results[type_model]['sharing_aux']['lambdas'] = lambda_\n",
    "        else: \n",
    "            results[type_model]['sharing_NOaux']['Acc'] = acc\n",
    "            results[type_model]['sharing_NOaux']['eta'] = eta\n",
    "            results[type_model]['sharing_NOaux']['hidden'] = hidden\n",
    "            results[type_model]['sharing_NOaux']['lambdas'] = lambda_\n",
    "    else:\n",
    "        if(aux_flag):\n",
    "            results[type_model]['NOsharing_aux']['Acc'] = acc\n",
    "            results[type_model]['NOsharing_aux']['eta'] = eta\n",
    "            results[type_model]['NOsharing_aux']['hidden'] = hidden\n",
    "            results[type_model]['NOsharing_aux']['lambdas'] = lambda_\n",
    "        else: \n",
    "            results[type_model]['NOsharing_NOaux']['Acc'] = acc\n",
    "            results[type_model]['NOsharing_NOaux']['eta'] = eta\n",
    "            results[type_model]['NOsharing_NOaux']['hidden'] = hidden\n",
    "            results[type_model]['NOsharing_NOaux']['lambdas'] = lambda_\n",
    "    return results\n",
    "\n",
    "def grid_search_(lambdas, etas, hidden_units, train_input, train_target, test_input, test_target):\n",
    "    type_models = ['Shallow', 'MLP', 'Deep1', 'Deep2']\n",
    "    sharing_flags = [True, False]\n",
    "    aux_flags = [True, False]\n",
    "    acc_test = torch.zeros(len(type_models),len(sharing_flags),len(aux_flags))\n",
    "    results = create_dict()\n",
    "    \n",
    "    i = 0\n",
    "\n",
    "    for t, type_model in enumerate(type_models):\n",
    "        for s, sharing_flag in enumerate(sharing_flags):\n",
    "            for a, aux_flag in enumerate(aux_flags):\n",
    "                i += 1\n",
    "                print(i, '/ 16')\n",
    "                print('Training architecture...')\n",
    "                performances = torch.zeros(len(lambdas),len(hidden_units),len(etas))\n",
    "                for l, lambda_ in enumerate(lambdas):\n",
    "                    for h, hidden in enumerate(hidden_units):\n",
    "                        for e, eta in enumerate(etas):\n",
    "                            acc, _ = predictions(train_input, train_target, hidden.item(), eta.item(), \n",
    "                                              lambda_.item(), model_type = type_model, \n",
    "                                              sharing = sharing_flag, aux = aux_flag)\n",
    "                            performances[l,h,e] = acc\n",
    "                best_performance = torch.max(performances)\n",
    "                best_idx = (performances == best_performance).nonzero();\n",
    "                \n",
    "                best_eta = etas[best_idx[0,2]].item()\n",
    "                best_hidden = hidden_units[best_idx[0,1]].item()\n",
    "                best_lambda = lambdas[best_idx[0,0]].item()\n",
    "                \n",
    "                results = fill_results(results, type_model, sharing_flag, aux_flag, \n",
    "                                       best_performance.item(), best_eta, best_hidden, best_lambda)\n",
    "                \n",
    "                print('Testing architecture...')\n",
    "                _, model = predictions(train_input, train_target, best_hidden, best_eta, best_lambda, \n",
    "                                      model_type = type_model, sharing = sharing_flag, aux = aux_flag)\n",
    "                \n",
    "                if(aux_flag):\n",
    "                    acc_test[t,s,a] = 1 - compute_nb_errors_aux(model, test_input, test_target, 100)/len(test_target)\n",
    "                else: \n",
    "                    acc_test[t,s,a] = 1 - compute_nb_errors_NOaux(model, test_input, test_target, 100)/len(test_target)\n",
    "    return results, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 16\n",
      "Training architecture...\n",
      "Testing architecture...\n",
      "2 / 16\n",
      "Training architecture...\n",
      "Testing architecture...\n",
      "3 / 16\n",
      "Training architecture...\n",
      "Testing architecture...\n",
      "4 / 16\n",
      "Training architecture...\n",
      "Testing architecture...\n",
      "5 / 16\n",
      "Training architecture...\n",
      "Testing architecture...\n",
      "6 / 16\n",
      "Training architecture...\n",
      "Testing architecture...\n",
      "7 / 16\n",
      "Training architecture...\n",
      "Testing architecture...\n",
      "8 / 16\n",
      "Training architecture...\n",
      "Testing architecture...\n",
      "9 / 16\n",
      "Training architecture...\n",
      "Testing architecture...\n",
      "10 / 16\n",
      "Training architecture...\n",
      "Testing architecture...\n",
      "11 / 16\n",
      "Training architecture...\n",
      "Testing architecture...\n",
      "12 / 16\n",
      "Training architecture...\n",
      "Testing architecture...\n",
      "13 / 16\n",
      "Training architecture...\n"
     ]
    }
   ],
   "source": [
    "lambdas = torch.tensor([0.25, 0.5, 0.75, 1])\n",
    "etas = torch.tensor([1e-3, 1e-2, 1e-1])\n",
    "hidden_units = torch.tensor([50, 100, 200, 300])\n",
    "\n",
    "results, acc_test = grid_search_(lambdas, etas, hidden_units, train_input, train_target, test_input, test_target)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
