{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x10f6da358>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_disc_set(nb):\n",
    "    input_ = torch.empty(nb, 2).uniform_(0,1)\n",
    "    target_ = torch.empty(nb).long()\n",
    "    \n",
    "    for i in range(nb):\n",
    "        if (torch.norm(input_[i]) < math.sqrt(1/(2*math.pi))):\n",
    "            target_[i] = 1\n",
    "        else : target_[i] = 0\n",
    "    \n",
    "    return input_, target_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_target = generate_disc_set(1000)\n",
    "test_input, test_target = generate_disc_set(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tanh_fun(x):\n",
    "    return x.tanh()\n",
    "    \n",
    "def d_Tanh(x):\n",
    "    return (1 - torch.pow(Tanh_fun(x), 2))\n",
    "\n",
    "def ReLU_fun(x):\n",
    "    return x * (x > 0).float()\n",
    "\n",
    "def d_ReLU(x):\n",
    "    return 1. * (x > 0).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LossMSE(v, t):\n",
    "    return torch.sum(torch.pow(t-v, 2)).item()\n",
    "    \n",
    "def d_LossMSE(v, t):\n",
    "    return (2*(v-t)).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward and backward passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    \n",
    "    def forward_pass(self, *input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward_pass(self, *gradwrtoutput):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        self.w = torch.empty(self.out_features, self.in_features).normal_(0,1e-6)\n",
    "        self.b = torch.empty(self.out_features).normal_(0,1e-6)\n",
    "        self.dl_dw = torch.zeros(self.w.size())\n",
    "        self.dl_db = torch.zeros(self.b.size())\n",
    "        \n",
    "        self.cache_forward = None\n",
    "    \n",
    "    def forward_pass(self, input):\n",
    "        self.cache_forward = input\n",
    "        return self.w @ input + self.b\n",
    "\n",
    "    def backward_pass(self, gradwrtoutput):\n",
    "        self.dl_dw += gradwrtoutput.view(self.out_features,1) @ self.cache_forward.view(1,self.in_features)\n",
    "        self.dl_db += gradwrtoutput\n",
    "        self.cache_forward = None\n",
    "        return gradwrtoutput @ self.w\n",
    "        \n",
    "    def param(self):\n",
    "        return [(self.w, self.dl_dw), (self.b, self.dl_db)]\n",
    "    \n",
    "    def zerograd(self):\n",
    "        self.dl_dw = torch.zeros(self.w.size())\n",
    "        self.dl_db = torch.zeros(self.b.size())\n",
    "        \n",
    "    def update(self, eta):\n",
    "        self.w -= eta * self.dl_dw\n",
    "        self.b -= eta * self.dl_db\n",
    "    \n",
    "\n",
    "class ReLU(Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "        self.cache_forward = None\n",
    "    \n",
    "    def forward_pass(self, input):\n",
    "        self.cache_forward = input\n",
    "        return ReLU_fun(input)\n",
    "\n",
    "    def backward_pass(self, gradwrtoutput):\n",
    "        dl_s = d_ReLU(self.cache_forward) * gradwrtoutput\n",
    "        self.cache_forward = None\n",
    "        return dl_s\n",
    "        \n",
    "    def param(self):\n",
    "        return []\n",
    "    \n",
    "    def zerograd(self):\n",
    "        return []\n",
    "\n",
    "class Tanh(Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Tanh, self).__init__()\n",
    "        self.cache_forward = None\n",
    "    \n",
    "    def forward_pass(self, input):\n",
    "        self.cache_forward = input\n",
    "        return Tanh_fun(input)\n",
    "\n",
    "    def backward_pass(self, gradwrtoutput):\n",
    "        dl_s = d_Tanh(self.cache_forward) * gradwrtoutput\n",
    "        self.cache_forward = None\n",
    "        return dl_s\n",
    "        \n",
    "    def param(self):\n",
    "        return []\n",
    "    \n",
    "    def zerograd(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "        super(Sequential, self).__init__()\n",
    "        self.modules = []\n",
    "        self.backward = None\n",
    "        for module in args:\n",
    "            self.modules.append(module)\n",
    "    \n",
    "    def forward_pass(self, input):\n",
    "        self.forward = input\n",
    "        for module in self.modules:\n",
    "            self.forward = module.forward_pass(self.forward)\n",
    "        return torch.argmax(self.forward)\n",
    "            \n",
    "    def backward_pass(self, target):\n",
    "        self.backward = d_LossMSE(torch.argmax(self.forward).item(), target)\n",
    "        for module in reversed(self.modules):\n",
    "            self.backward = module.backward_pass(self.backward)\n",
    "    \n",
    "    def zerograd(self):\n",
    "        for module in self.modules:\n",
    "            module.zerograd()\n",
    "            \n",
    "    def update(self, eta):\n",
    "        for module in self.modules:\n",
    "            if(len(module.param()) > 0):\n",
    "                module.update(eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_input, train_target, eta, mini_batch_size, epochs):\n",
    "    losses = []\n",
    "    for e in range(epochs):\n",
    "        total_loss = 0\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            for i in range(mini_batch_size):\n",
    "                output = model.forward_pass(train_input[b+i])\n",
    "                model.backward_pass(train_target[b+i])\n",
    "                total_loss += LossMSE(output, train_target[b+i])\n",
    "            model.update(eta)\n",
    "            model.zerograd()\n",
    "        losses.append(total_loss)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15,\n",
       " 10,\n",
       " 14,\n",
       " 14,\n",
       " 85,\n",
       " 83,\n",
       " 83,\n",
       " 92,\n",
       " 90,\n",
       " 91,\n",
       " 85,\n",
       " 90,\n",
       " 86,\n",
       " 86,\n",
       " 85,\n",
       " 83,\n",
       " 83,\n",
       " 92,\n",
       " 90,\n",
       " 91,\n",
       " 85,\n",
       " 90,\n",
       " 86,\n",
       " 86,\n",
       " 85,\n",
       " 83,\n",
       " 83,\n",
       " 92,\n",
       " 90,\n",
       " 91,\n",
       " 85,\n",
       " 90,\n",
       " 86,\n",
       " 86,\n",
       " 85,\n",
       " 83,\n",
       " 83,\n",
       " 92,\n",
       " 90,\n",
       " 91,\n",
       " 85,\n",
       " 90,\n",
       " 86,\n",
       " 86,\n",
       " 85,\n",
       " 83,\n",
       " 83,\n",
       " 92,\n",
       " 90,\n",
       " 91,\n",
       " 85,\n",
       " 90,\n",
       " 86,\n",
       " 86,\n",
       " 85,\n",
       " 83,\n",
       " 83,\n",
       " 92,\n",
       " 90,\n",
       " 91,\n",
       " 85,\n",
       " 90,\n",
       " 86,\n",
       " 86,\n",
       " 85,\n",
       " 83,\n",
       " 83,\n",
       " 92,\n",
       " 90,\n",
       " 91,\n",
       " 85,\n",
       " 90,\n",
       " 86,\n",
       " 86,\n",
       " 85,\n",
       " 83,\n",
       " 83,\n",
       " 92,\n",
       " 90,\n",
       " 91,\n",
       " 85,\n",
       " 90,\n",
       " 86,\n",
       " 86,\n",
       " 85,\n",
       " 83,\n",
       " 83,\n",
       " 92,\n",
       " 90,\n",
       " 91,\n",
       " 85,\n",
       " 90,\n",
       " 86,\n",
       " 86,\n",
       " 85,\n",
       " 83,\n",
       " 83,\n",
       " 92,\n",
       " 90,\n",
       " 91]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta = 0.01\n",
    "mini_batch_size = 100\n",
    "epochs = 10\n",
    "\n",
    "model = Sequential(Linear(2,25), ReLU(), Linear(25,25), ReLU(), Linear(25,2), Tanh())\n",
    "train(model, train_input, train_target, eta, mini_batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
